* input layer (입력층) : 입력 뉴런들로 구성된 층

* Output layer (출력층) : 결과물을 생산하는 출력 뉴런들로 구성된 층

* Hidden layer (은닉층) : 입력층과 출력 층 사이 구성하고 있는 레이어

* Deep Neural Network (DNN) : 3개 층 이상(2개 이상의 Hidden layer) 으로 구성된 신경망. 

* 신경망 학습 순서
1. 데이터를 받는다
2. 층에 도달 시 각각의 가중치(weight)를 사용하여 출력 값을 계산한다.
3. 모든 층에서 반복, 마지막으로 출력층에 예측값이 출력됨
4. 예측값(output layer에서 도출된 값)과 실제값을 비교하는 손실함수를 이용하여 점수를 계산
5. 해당 점수를 피드백으로 삼아, 손실점수가 감소되는 방향으로 다시 가중치를 수정 (optimizer)
6. 위의 과정을 반복함(Loop)
7. 손실함수를 통해 계산되는 손실점수를 최소화 시키는 가중치를 찾아 냄

* 가중치(weight) : 뉴런사이의 연결 강도를 나타내며, 신경망이 훈련을 하는 동안 업데이트 되어 weight가 변경됨.

* Learnable parameter라고도 부름

* bias : Bias 뉴런의 가중치를 말함. W1*x1 + bias1

* Activation Function (활성함수) : 주어진 입력 값들을 받은 뉴런의 출력값을 돌려주는 함수. 각 함수의 특성에 따라 이용하는 함수의 종류가 다름
1. Sigmoid Function : 초창기에 많이 사용된 활성함수. S자 곡선을 가지는 함수. y의 값이 [0,1] 이므로 그에 상응되는 x값으로 조정해 주어야 한다(normalize)
                      AND,OR,XOR 연산을 다루는데 적합한 함수
                      Binary Classification에 적합한 함수
                      
2. Hyperbolic Tangent (Tanh) : 시그모이드의 S자 곡선을 가진 형태이며, 출력값의 범위는 [-1,1]

3. ReLU : Sigmoid의 대안으로 나온 함수이며, 0이하의 수는 0, 양수인 경우에는 양수의 값을 그대로 출력(y=x)하여 왜곡이 적어짐. y = [0,inf]

